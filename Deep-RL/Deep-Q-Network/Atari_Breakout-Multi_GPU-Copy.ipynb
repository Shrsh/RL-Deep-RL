{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "Ydi0qFEfMA2l",
    "outputId": "d7d7d1c6-a5ed-431a-b5ac-cb65453ca48c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import random\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import datetime\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-QrX45aMhXe"
   },
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")\n",
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m9ajWOrjMkxP"
   },
   "outputs": [],
   "source": [
    "def state_processer(input):\n",
    "    input = tf.image.rgb_to_grayscale(input, name=None)\n",
    "    input = tf.image.crop_to_bounding_box(input,10,0,160,160)\n",
    "    input = tf.image.resize(input,(84,84),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return tf.squeeze(input)\n",
    "  \n",
    "class My_Model(Model):\n",
    "  def __init__(self):\n",
    "    \n",
    "    super(My_Model, self).__init__()\n",
    "    self.conv1 = Conv2D(16, 8,strides=(4,4) ,activation='relu')\n",
    "    self.conv2 = Conv2D(32, 4,strides=(2,2) ,activation='relu')\n",
    "    self.conv3 = Conv2D(64, 3,strides=(1,1) ,activation='relu')\n",
    "    self.flatten = Flatten()\n",
    "    self.d1 = Dense(512, activation='relu')\n",
    "    self.d2 = Dense(len(VALID_ACTIONS))\n",
    "\n",
    "  def call(self, x):\n",
    "#     with strategy.scope():\n",
    "    x = self.conv1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.d1(x)\n",
    "    return self.d2(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JeFSMlOwhbTy"
   },
   "outputs": [],
   "source": [
    "#defining loss and optimizers\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "\n",
    "def loss_function(target_y, predicted_y):\n",
    "  return tf.keras.losses.MSE(target_y, predicted_y)\n",
    "  \n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=True)\n",
    "    loss = loss(labels, predictions)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LApMIkC3t_-2"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(estimator, state, epsilon,env):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        state: processed state \n",
    "    Returns:\n",
    "        Action based on the epsilon greedy policy\n",
    "    \"\"\"\n",
    "    x = random.uniform(0, 1)\n",
    "    if x < epsilon:\n",
    "      return env.action_space.sample()\n",
    "    else:\n",
    "      state = tf.cast(np.reshape(state, (1,84,84,4)), tf.float32)\n",
    "      q_values = np.array(estimator.predict(state))\n",
    "      # print(\"Everything's working\")\n",
    "      return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pnSvaajshiFR"
   },
   "outputs": [],
   "source": [
    "def deep_q_learning(env,\n",
    "                    total_t,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size_per_device=32,\n",
    "                    record_video_every=3000,\n",
    "                    save_weights_every=100,\n",
    "                    number_of_epochs = 16,\n",
    "                    ):\n",
    "    BUFFER_SIZE = 1000\n",
    "    BATCH_SIZE_PER_REPLICA = batch_size_per_device\n",
    "    batch_size = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "    # Keeps track of useful statistics\n",
    "    # like episode length, episode reward\n",
    "    stats = {\"episode_lengths\":np.zeros(num_episodes), \"episode_rewards\":np.zeros(num_episodes)}\n",
    "    # average episode reward for past n episodes\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_path =os.path.abspath(os.path.join(experiment_dir, \"checkpoint/cp-{epoch:04d}.ckpt\"))\n",
    "    summary_path   = os.path.join(experiment_dir, \"summary\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    reward_file_path = os.path.abspath(os.path.join(experiment_dir,\"Rewards.txt\"))\n",
    "\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        os.makedirs(checkpoint_path)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "    if not os.path.exists(summary_path):\n",
    "        os.makedirs(summary_path)\n",
    "        \n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=0,\n",
    "                                                 save_freq = number_of_epochs)\n",
    "    with strategy.scope():\n",
    "        target_estimator = My_Model()\n",
    "        q_estimator = My_Model()\n",
    "        q_estimator.compile(optimizer='adam',\n",
    "                  loss=loss_function,\n",
    "                  metrics=['mse'])\n",
    "        target_estimator.compile(optimizer='adam',\n",
    "                  loss=loss_function,\n",
    "                  metrics=['mse'])\n",
    "    \n",
    "    # Save the weights using the `checkpoint_path` format\n",
    "    q_estimator.save_weights(checkpoint_path.format(epoch=0))\n",
    "    \n",
    "    latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest:\n",
    "        print(\"Loading Weights from last checkpoint\")\n",
    "        q_estimator.load_weights(latest)\n",
    "            \n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "    \n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = tf.image.convert_image_dtype(state, 'float32', saturate=False, name=None)\n",
    "    state = state_processer(state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action = epsilon_greedy_policy(q_estimator, state, epsilons[min(total_t,epsilon_decay_steps-1)],env)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = tf.image.convert_image_dtype(next_state, 'float32', saturate=False, name=None)\n",
    "        next_state = state_processer(next_state)\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processer(state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    # Record videos\n",
    "    env= gym.wrappers.Monitor(env,\n",
    "                 directory=monitor_path,\n",
    "                 resume=True,\n",
    "                 video_callable=lambda count: total_t % record_video_every == 0)\n",
    "    \n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "      # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = tf.image.convert_image_dtype(state, 'float32', saturate=False, name=None)\n",
    "        state = state_processer(state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        # loss = None\n",
    "         # One step in the environment\n",
    "        for t in itertools.count():\n",
    "          # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "            # TODO: Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "#                 q_estimator.save_weights(checkpoint_path + '/model')\n",
    "                latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "                target_estimator.load_weights(latest)\n",
    "        \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, epsilon= {}, reward = {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes,epsilon,stats['episode_rewards'][i_episode]), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step in the environment\n",
    "            action = epsilon_greedy_policy(q_estimator, state, epsilon,env)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = tf.image.convert_image_dtype(next_state, 'float32', saturate=False, name=None)\n",
    "            next_state = state_processer(next_state)\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            #Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "            # Update statistics\n",
    "            stats['episode_rewards'][i_episode] += reward\n",
    "            stats['episode_lengths'][i_episode] = t\n",
    "\n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "            #Calculate q values and targets\n",
    "            q_values_next = target_estimator.predict(next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "            # states_batch = np.reshape(states_batch, (batch_size,84,84,4,1))\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((np.asarray(states_batch),np.asarray(targets_batch)))\n",
    "            dataset = dataset.cache().shuffle(BUFFER_SIZE).batch(batch_size)\n",
    "            with strategy.scope():\n",
    "                q_estimator.fit(dataset,epochs=number_of_epochs,verbose=0,callbacks=[cp_callback],batch_size=16)\n",
    "            total_t += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state \n",
    "        file_object = open(reward_file_path, 'a')\n",
    "        file_object.write(str(stats['episode_rewards'][i_episode]) + ',' +str(epsilon) + str(total_t) + '\\n')\n",
    "    q_estimator.summary()\n",
    "    # Display the model's architecture\n",
    "    return stats\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "Pw-hc16SYmcQ",
    "outputId": "0c52057b-01a5-4f6c-8080-f8335566d8e1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Weights from last checkpoint\n",
      "Populating replay memory...\n",
      "Step 0 (0) @ Episode 1/1500, epsilon= 0.99, reward = 0.0INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.224106). Check your callbacks.\n",
      "Step 1 (1) @ Episode 1/1500, epsilon= 0.98999821999644, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.239408). Check your callbacks.\n",
      "Step 2 (2) @ Episode 1/1500, epsilon= 0.98999643999288, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.323432). Check your callbacks.\n",
      "Step 3 (3) @ Episode 1/1500, epsilon= 0.98999465998932, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.224550). Check your callbacks.\n",
      "Step 4 (4) @ Episode 1/1500, epsilon= 0.9899928799857599, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.369220). Check your callbacks.\n",
      "Step 5 (5) @ Episode 1/1500, epsilon= 0.9899910999821999, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.220560). Check your callbacks.\n",
      "Step 6 (6) @ Episode 1/1500, epsilon= 0.9899893199786399, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.222807). Check your callbacks.\n",
      "Step 7 (7) @ Episode 1/1500, epsilon= 0.9899875399750799, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.225397). Check your callbacks.\n",
      "Step 8 (8) @ Episode 1/1500, epsilon= 0.9899857599715199, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.221071). Check your callbacks.\n",
      "Step 9 (9) @ Episode 1/1500, epsilon= 0.98998397996796, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.220709). Check your callbacks.\n",
      "Step 10 (10) @ Episode 1/1500, epsilon= 0.9899821999644, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.224336). Check your callbacks.\n",
      "Step 11 (11) @ Episode 1/1500, epsilon= 0.98998041996084, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.225724). Check your callbacks.\n",
      "Step 12 (12) @ Episode 1/1500, epsilon= 0.9899786399572799, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.226817). Check your callbacks.\n",
      "Step 13 (13) @ Episode 1/1500, epsilon= 0.9899768599537199, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.223106). Check your callbacks.\n",
      "Step 14 (14) @ Episode 1/1500, epsilon= 0.9899750799501599, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.220338). Check your callbacks.\n",
      "Step 15 (15) @ Episode 1/1500, epsilon= 0.9899732999465999, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.222154). Check your callbacks.\n",
      "Step 16 (16) @ Episode 1/1500, epsilon= 0.9899715199430399, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.232121). Check your callbacks.\n",
      "Step 17 (17) @ Episode 1/1500, epsilon= 0.9899697399394799, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.222648). Check your callbacks.\n",
      "Step 18 (18) @ Episode 1/1500, epsilon= 0.9899679599359199, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.229450). Check your callbacks.\n",
      "Step 19 (19) @ Episode 1/1500, epsilon= 0.9899661799323598, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.243285). Check your callbacks.\n",
      "Step 20 (20) @ Episode 1/1500, epsilon= 0.9899643999287998, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.291469). Check your callbacks.\n",
      "Step 21 (21) @ Episode 1/1500, epsilon= 0.9899626199252398, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.225174). Check your callbacks.\n",
      "Step 22 (22) @ Episode 1/1500, epsilon= 0.9899608399216798, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.329059). Check your callbacks.\n",
      "Step 23 (23) @ Episode 1/1500, epsilon= 0.9899590599181198, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.220912). Check your callbacks.\n",
      "Step 24 (24) @ Episode 1/1500, epsilon= 0.9899572799145598, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.228667). Check your callbacks.\n",
      "Step 25 (25) @ Episode 1/1500, epsilon= 0.9899554999109998, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.224305). Check your callbacks.\n",
      "Step 26 (26) @ Episode 1/1500, epsilon= 0.9899537199074399, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.226011). Check your callbacks.\n",
      "Step 27 (27) @ Episode 1/1500, epsilon= 0.9899519399038798, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.221176). Check your callbacks.\n",
      "Step 28 (28) @ Episode 1/1500, epsilon= 0.9899501599003198, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (4.510302). Check your callbacks.\n",
      "Step 29 (29) @ Episode 1/1500, epsilon= 0.9899483798967598, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (4.092303). Check your callbacks.\n",
      "Step 30 (30) @ Episode 1/1500, epsilon= 0.9899465998931998, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.229918). Check your callbacks.\n",
      "Step 31 (31) @ Episode 1/1500, epsilon= 0.9899448198896398, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.223361). Check your callbacks.\n",
      "Step 32 (32) @ Episode 1/1500, epsilon= 0.9899430398860798, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.221279). Check your callbacks.\n",
      "Step 33 (33) @ Episode 1/1500, epsilon= 0.9899412598825198, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.253565). Check your callbacks.\n",
      "Step 34 (34) @ Episode 1/1500, epsilon= 0.9899394798789597, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.226648). Check your callbacks.\n",
      "Step 35 (35) @ Episode 1/1500, epsilon= 0.9899376998753997, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (3.414966). Check your callbacks.\n",
      "Step 36 (36) @ Episode 1/1500, epsilon= 0.9899359198718397, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (3.319223). Check your callbacks.\n",
      "Step 37 (37) @ Episode 1/1500, epsilon= 0.9899341398682797, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.227154). Check your callbacks.\n",
      "Step 38 (38) @ Episode 1/1500, epsilon= 0.9899323598647197, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.234952). Check your callbacks.\n",
      "Step 39 (39) @ Episode 1/1500, epsilon= 0.9899305798611597, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.434808). Check your callbacks.\n",
      "Step 40 (40) @ Episode 1/1500, epsilon= 0.9899287998575997, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.360382). Check your callbacks.\n",
      "Step 41 (41) @ Episode 1/1500, epsilon= 0.9899270198540397, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.225942). Check your callbacks.\n",
      "Step 42 (42) @ Episode 1/1500, epsilon= 0.9899252398504796, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.225525). Check your callbacks.\n",
      "Step 43 (43) @ Episode 1/1500, epsilon= 0.9899234598469197, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.223020). Check your callbacks.\n",
      "Step 44 (44) @ Episode 1/1500, epsilon= 0.9899216798433597, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.224568). Check your callbacks.\n",
      "Step 45 (45) @ Episode 1/1500, epsilon= 0.9899198998397997, reward = 0.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.220214). Check your callbacks.\n",
      "Step 46 (46) @ Episode 1/1500, epsilon= 0.9899181198362397, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.238240). Check your callbacks.\n",
      "Step 47 (47) @ Episode 1/1500, epsilon= 0.9899163398326797, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.230061). Check your callbacks.\n",
      "Step 48 (48) @ Episode 1/1500, epsilon= 0.9899145598291197, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.220409). Check your callbacks.\n",
      "Step 49 (49) @ Episode 1/1500, epsilon= 0.9899127798255597, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.226152). Check your callbacks.\n",
      "Step 50 (50) @ Episode 1/1500, epsilon= 0.9899109998219996, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.231954). Check your callbacks.\n",
      "Step 51 (51) @ Episode 1/1500, epsilon= 0.9899092198184396, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.312245). Check your callbacks.\n",
      "Step 52 (52) @ Episode 1/1500, epsilon= 0.9899074398148796, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.223388). Check your callbacks.\n",
      "Step 53 (53) @ Episode 1/1500, epsilon= 0.9899056598113196, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.223770). Check your callbacks.\n",
      "Step 54 (54) @ Episode 1/1500, epsilon= 0.9899038798077596, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.244120). Check your callbacks.\n",
      "Step 55 (55) @ Episode 1/1500, epsilon= 0.9899020998041996, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.224975). Check your callbacks.\n",
      "Step 56 (56) @ Episode 1/1500, epsilon= 0.9899003198006396, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.221923). Check your callbacks.\n",
      "Step 57 (57) @ Episode 1/1500, epsilon= 0.9898985397970795, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.376233). Check your callbacks.\n",
      "Step 58 (58) @ Episode 1/1500, epsilon= 0.9898967597935195, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.221504). Check your callbacks.\n",
      "Step 59 (59) @ Episode 1/1500, epsilon= 0.9898949797899596, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.226932). Check your callbacks.\n",
      "Step 60 (60) @ Episode 1/1500, epsilon= 0.9898931997863996, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.220924). Check your callbacks.\n",
      "Step 61 (61) @ Episode 1/1500, epsilon= 0.9898914197828396, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.222642). Check your callbacks.\n",
      "Step 62 (62) @ Episode 1/1500, epsilon= 0.9898896397792796, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.224345). Check your callbacks.\n",
      "Step 63 (63) @ Episode 1/1500, epsilon= 0.9898878597757196, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.219663). Check your callbacks.\n",
      "Step 64 (64) @ Episode 1/1500, epsilon= 0.9898860797721596, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.223479). Check your callbacks.\n",
      "Step 65 (65) @ Episode 1/1500, epsilon= 0.9898842997685995, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.226552). Check your callbacks.\n",
      "Step 66 (66) @ Episode 1/1500, epsilon= 0.9898825197650395, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.219815). Check your callbacks.\n",
      "Step 67 (67) @ Episode 1/1500, epsilon= 0.9898807397614795, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.221974). Check your callbacks.\n",
      "Step 68 (68) @ Episode 1/1500, epsilon= 0.9898789597579195, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.221672). Check your callbacks.\n",
      "Step 69 (69) @ Episode 1/1500, epsilon= 0.9898771797543595, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.217622). Check your callbacks.\n",
      "Step 70 (70) @ Episode 1/1500, epsilon= 0.9898753997507995, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.225863). Check your callbacks.\n",
      "Step 71 (71) @ Episode 1/1500, epsilon= 0.9898736197472395, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.226054). Check your callbacks.\n",
      "Step 72 (72) @ Episode 1/1500, epsilon= 0.9898718397436794, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.243639). Check your callbacks.\n",
      "Step 73 (73) @ Episode 1/1500, epsilon= 0.9898700597401194, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.226594). Check your callbacks.\n",
      "Step 74 (74) @ Episode 1/1500, epsilon= 0.9898682797365594, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.221475). Check your callbacks.\n",
      "Step 75 (75) @ Episode 1/1500, epsilon= 0.9898664997329994, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.568542). Check your callbacks.\n",
      "Step 76 (76) @ Episode 1/1500, epsilon= 0.9898647197294395, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.234832). Check your callbacks.\n",
      "Step 77 (77) @ Episode 1/1500, epsilon= 0.9898629397258795, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.271030). Check your callbacks.\n",
      "Step 78 (78) @ Episode 1/1500, epsilon= 0.9898611597223195, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.224728). Check your callbacks.\n",
      "Step 79 (79) @ Episode 1/1500, epsilon= 0.9898593797187595, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.223329). Check your callbacks.\n",
      "Step 80 (80) @ Episode 1/1500, epsilon= 0.9898575997151994, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.225407). Check your callbacks.\n",
      "Step 81 (81) @ Episode 1/1500, epsilon= 0.9898558197116394, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.306372). Check your callbacks.\n",
      "Step 82 (82) @ Episode 1/1500, epsilon= 0.9898540397080794, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.226056). Check your callbacks.\n",
      "Step 83 (83) @ Episode 1/1500, epsilon= 0.9898522597045194, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.232422). Check your callbacks.\n",
      "Step 84 (84) @ Episode 1/1500, epsilon= 0.9898504797009594, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.220255). Check your callbacks.\n",
      "Step 85 (85) @ Episode 1/1500, epsilon= 0.9898486996973994, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.223668). Check your callbacks.\n",
      "Step 86 (86) @ Episode 1/1500, epsilon= 0.9898469196938394, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.223320). Check your callbacks.\n",
      "Step 87 (87) @ Episode 1/1500, epsilon= 0.9898451396902793, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.624768). Check your callbacks.\n",
      "Step 88 (88) @ Episode 1/1500, epsilon= 0.9898433596867193, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.228254). Check your callbacks.\n",
      "Step 89 (89) @ Episode 1/1500, epsilon= 0.9898415796831593, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.221100). Check your callbacks.\n",
      "Step 90 (90) @ Episode 1/1500, epsilon= 0.9898397996795993, reward = 1.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.221132). Check your callbacks.\n",
      "Step 91 (91) @ Episode 1/1500, epsilon= 0.9898380196760393, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.223560). Check your callbacks.\n",
      "Step 92 (92) @ Episode 1/1500, epsilon= 0.9898362396724794, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.227685). Check your callbacks.\n",
      "Step 93 (93) @ Episode 1/1500, epsilon= 0.9898344596689194, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.222468). Check your callbacks.\n",
      "Step 94 (94) @ Episode 1/1500, epsilon= 0.9898326796653594, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.222253). Check your callbacks.\n",
      "Step 95 (95) @ Episode 1/1500, epsilon= 0.9898308996617993, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.222898). Check your callbacks.\n",
      "Step 96 (96) @ Episode 1/1500, epsilon= 0.9898291196582393, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.222270). Check your callbacks.\n",
      "Step 97 (97) @ Episode 1/1500, epsilon= 0.9898273396546793, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.232133). Check your callbacks.\n",
      "Step 98 (98) @ Episode 1/1500, epsilon= 0.9898255596511193, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.228183). Check your callbacks.\n",
      "Step 99 (99) @ Episode 1/1500, epsilon= 0.9898237796475593, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.225791). Check your callbacks.\n",
      "Step 100 (100) @ Episode 1/1500, epsilon= 0.9898219996439993, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.223781). Check your callbacks.\n",
      "Step 101 (101) @ Episode 1/1500, epsilon= 0.9898202196404393, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.221420). Check your callbacks.\n",
      "Step 102 (102) @ Episode 1/1500, epsilon= 0.9898184396368792, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.225053). Check your callbacks.\n",
      "Step 103 (103) @ Episode 1/1500, epsilon= 0.9898166596333192, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.226407). Check your callbacks.\n",
      "Step 104 (104) @ Episode 1/1500, epsilon= 0.9898148796297592, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.416667). Check your callbacks.\n",
      "Step 105 (105) @ Episode 1/1500, epsilon= 0.9898130996261992, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.223047). Check your callbacks.\n",
      "Step 106 (106) @ Episode 1/1500, epsilon= 0.9898113196226392, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.224748). Check your callbacks.\n",
      "Step 107 (107) @ Episode 1/1500, epsilon= 0.9898095396190792, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.218214). Check your callbacks.\n",
      "Step 108 (108) @ Episode 1/1500, epsilon= 0.9898077596155193, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.276605). Check your callbacks.\n",
      "Step 109 (109) @ Episode 1/1500, epsilon= 0.9898059796119593, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.382936). Check your callbacks.\n",
      "Step 110 (110) @ Episode 1/1500, epsilon= 0.9898041996083992, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.330178). Check your callbacks.\n",
      "Step 111 (111) @ Episode 1/1500, epsilon= 0.9898024196048392, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.228611). Check your callbacks.\n",
      "Step 112 (112) @ Episode 1/1500, epsilon= 0.9898006396012792, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.241301). Check your callbacks.\n",
      "Step 113 (113) @ Episode 1/1500, epsilon= 0.9897988595977192, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.605513). Check your callbacks.\n",
      "Step 114 (114) @ Episode 1/1500, epsilon= 0.9897970795941592, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.232935). Check your callbacks.\n",
      "Step 115 (115) @ Episode 1/1500, epsilon= 0.9897952995905992, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.224795). Check your callbacks.\n",
      "Step 116 (116) @ Episode 1/1500, epsilon= 0.9897935195870392, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.586522). Check your callbacks.\n",
      "Step 117 (117) @ Episode 1/1500, epsilon= 0.9897917395834792, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.367935). Check your callbacks.\n",
      "Step 118 (118) @ Episode 1/1500, epsilon= 0.9897899595799191, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.232386). Check your callbacks.\n",
      "Step 119 (119) @ Episode 1/1500, epsilon= 0.9897881795763591, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.541847). Check your callbacks.\n",
      "Step 120 (120) @ Episode 1/1500, epsilon= 0.9897863995727991, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.233146). Check your callbacks.\n",
      "Step 121 (121) @ Episode 1/1500, epsilon= 0.9897846195692391, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.227427). Check your callbacks.\n",
      "Step 122 (122) @ Episode 1/1500, epsilon= 0.9897828395656791, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.556979). Check your callbacks.\n",
      "Step 123 (123) @ Episode 1/1500, epsilon= 0.9897810595621191, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.234179). Check your callbacks.\n",
      "Step 124 (124) @ Episode 1/1500, epsilon= 0.9897792795585592, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.228235). Check your callbacks.\n",
      "Step 125 (125) @ Episode 1/1500, epsilon= 0.989777499554999, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.224381). Check your callbacks.\n",
      "Step 126 (126) @ Episode 1/1500, epsilon= 0.9897757195514391, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.252266). Check your callbacks.\n",
      "Step 127 (127) @ Episode 1/1500, epsilon= 0.9897739395478791, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.234992). Check your callbacks.\n",
      "Step 128 (128) @ Episode 1/1500, epsilon= 0.9897721595443191, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.397964). Check your callbacks.\n",
      "Step 129 (129) @ Episode 1/1500, epsilon= 0.9897703795407591, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.233848). Check your callbacks.\n",
      "Step 130 (130) @ Episode 1/1500, epsilon= 0.9897685995371991, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.232627). Check your callbacks.\n",
      "Step 131 (131) @ Episode 1/1500, epsilon= 0.9897668195336391, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.310212). Check your callbacks.\n",
      "Step 132 (132) @ Episode 1/1500, epsilon= 0.9897650395300791, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.260815). Check your callbacks.\n",
      "Step 133 (133) @ Episode 1/1500, epsilon= 0.989763259526519, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.575872). Check your callbacks.\n",
      "Step 134 (134) @ Episode 1/1500, epsilon= 0.989761479522959, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.227483). Check your callbacks.\n",
      "Step 135 (135) @ Episode 1/1500, epsilon= 0.989759699519399, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.226052). Check your callbacks.\n",
      "Step 136 (136) @ Episode 1/1500, epsilon= 0.989757919515839, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.223092). Check your callbacks.\n",
      "Step 137 (137) @ Episode 1/1500, epsilon= 0.989756139512279, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.256809). Check your callbacks.\n",
      "Step 138 (138) @ Episode 1/1500, epsilon= 0.989754359508719, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.249419). Check your callbacks.\n",
      "Step 139 (139) @ Episode 1/1500, epsilon= 0.989752579505159, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.321776). Check your callbacks.\n",
      "Step 140 (140) @ Episode 1/1500, epsilon= 0.9897507995015989, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.318280). Check your callbacks.\n",
      "Step 141 (141) @ Episode 1/1500, epsilon= 0.989749019498039, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.253993). Check your callbacks.\n",
      "Step 142 (142) @ Episode 1/1500, epsilon= 0.989747239494479, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.424296). Check your callbacks.\n",
      "Step 143 (143) @ Episode 1/1500, epsilon= 0.989745459490919, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.228781). Check your callbacks.\n",
      "Step 144 (144) @ Episode 1/1500, epsilon= 0.989743679487359, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.252319). Check your callbacks.\n",
      "Step 145 (145) @ Episode 1/1500, epsilon= 0.989741899483799, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.343534). Check your callbacks.\n",
      "Step 146 (146) @ Episode 1/1500, epsilon= 0.989740119480239, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.256777). Check your callbacks.\n",
      "Step 147 (147) @ Episode 1/1500, epsilon= 0.989738339476679, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.238746). Check your callbacks.\n",
      "Step 148 (148) @ Episode 1/1500, epsilon= 0.9897365594731189, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.230499). Check your callbacks.\n",
      "Step 149 (149) @ Episode 1/1500, epsilon= 0.9897347794695589, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.242119). Check your callbacks.\n",
      "Step 150 (150) @ Episode 1/1500, epsilon= 0.9897329994659989, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.246885). Check your callbacks.\n",
      "Step 151 (151) @ Episode 1/1500, epsilon= 0.9897312194624389, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.224128). Check your callbacks.\n",
      "Step 152 (152) @ Episode 1/1500, epsilon= 0.9897294394588789, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.254168). Check your callbacks.\n",
      "Step 153 (153) @ Episode 1/1500, epsilon= 0.9897276594553189, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.269056). Check your callbacks.\n",
      "Step 154 (154) @ Episode 1/1500, epsilon= 0.9897258794517589, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.219102). Check your callbacks.\n",
      "Step 155 (155) @ Episode 1/1500, epsilon= 0.9897240994481988, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.232425). Check your callbacks.\n",
      "Step 156 (156) @ Episode 1/1500, epsilon= 0.9897223194446388, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.225115). Check your callbacks.\n",
      "Step 157 (157) @ Episode 1/1500, epsilon= 0.9897205394410789, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.227221). Check your callbacks.\n",
      "Step 158 (158) @ Episode 1/1500, epsilon= 0.9897187594375189, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.232118). Check your callbacks.\n",
      "Step 159 (159) @ Episode 1/1500, epsilon= 0.9897169794339589, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.222091). Check your callbacks.\n",
      "Step 160 (160) @ Episode 1/1500, epsilon= 0.9897151994303989, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.229225). Check your callbacks.\n",
      "Step 161 (161) @ Episode 1/1500, epsilon= 0.9897134194268389, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.226594). Check your callbacks.\n",
      "Step 162 (162) @ Episode 1/1500, epsilon= 0.9897116394232789, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.230034). Check your callbacks.\n",
      "Step 163 (163) @ Episode 1/1500, epsilon= 0.9897098594197188, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.227474). Check your callbacks.\n",
      "Step 164 (164) @ Episode 1/1500, epsilon= 0.9897080794161588, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.230977). Check your callbacks.\n",
      "Step 165 (165) @ Episode 1/1500, epsilon= 0.9897062994125988, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.435084). Check your callbacks.\n",
      "Step 166 (166) @ Episode 1/1500, epsilon= 0.9897045194090388, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.584136). Check your callbacks.\n",
      "Step 167 (167) @ Episode 1/1500, epsilon= 0.9897027394054788, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.254734). Check your callbacks.\n",
      "Step 168 (168) @ Episode 1/1500, epsilon= 0.9897009594019188, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.262397). Check your callbacks.\n",
      "Step 169 (169) @ Episode 1/1500, epsilon= 0.9896991793983588, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.645213). Check your callbacks.\n",
      "Step 170 (170) @ Episode 1/1500, epsilon= 0.9896973993947987, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.277494). Check your callbacks.\n",
      "Step 171 (171) @ Episode 1/1500, epsilon= 0.9896956193912387, reward = 2.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.248883). Check your callbacks.\n",
      "Step 172 (172) @ Episode 1/1500, epsilon= 0.9896938393876787, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.505661). Check your callbacks.\n",
      "Step 173 (173) @ Episode 1/1500, epsilon= 0.9896920593841187, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.256004). Check your callbacks.\n",
      "Step 174 (174) @ Episode 1/1500, epsilon= 0.9896902793805588, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.407954). Check your callbacks.\n",
      "Step 175 (175) @ Episode 1/1500, epsilon= 0.9896884993769988, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.224270). Check your callbacks.\n",
      "Step 176 (176) @ Episode 1/1500, epsilon= 0.9896867193734388, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.225584). Check your callbacks.\n",
      "Step 177 (177) @ Episode 1/1500, epsilon= 0.9896849393698788, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.408923). Check your callbacks.\n",
      "Step 178 (178) @ Episode 1/1500, epsilon= 0.9896831593663187, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.230986). Check your callbacks.\n",
      "Step 179 (179) @ Episode 1/1500, epsilon= 0.9896813793627587, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.229783). Check your callbacks.\n",
      "Step 180 (180) @ Episode 1/1500, epsilon= 0.9896795993591987, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.224760). Check your callbacks.\n",
      "Step 181 (181) @ Episode 1/1500, epsilon= 0.9896778193556387, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.227992). Check your callbacks.\n",
      "Step 182 (182) @ Episode 1/1500, epsilon= 0.9896760393520787, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.232202). Check your callbacks.\n",
      "Step 183 (183) @ Episode 1/1500, epsilon= 0.9896742593485187, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.222495). Check your callbacks.\n",
      "Step 184 (184) @ Episode 1/1500, epsilon= 0.9896724793449587, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.240266). Check your callbacks.\n",
      "Step 185 (185) @ Episode 1/1500, epsilon= 0.9896706993413986, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.225266). Check your callbacks.\n",
      "Step 186 (186) @ Episode 1/1500, epsilon= 0.9896689193378386, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.223981). Check your callbacks.\n",
      "Step 187 (187) @ Episode 1/1500, epsilon= 0.9896671393342786, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.227158). Check your callbacks.\n",
      "Step 188 (188) @ Episode 1/1500, epsilon= 0.9896653593307186, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.228810). Check your callbacks.\n",
      "Step 189 (189) @ Episode 1/1500, epsilon= 0.9896635793271586, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.224145). Check your callbacks.\n",
      "Step 190 (190) @ Episode 1/1500, epsilon= 0.9896617993235987, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.223753). Check your callbacks.\n",
      "Step 191 (191) @ Episode 1/1500, epsilon= 0.9896600193200387, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.231409). Check your callbacks.\n",
      "Step 192 (192) @ Episode 1/1500, epsilon= 0.9896582393164787, reward = 3.0WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.227394). Check your callbacks.\n",
      "Step 193 (193) @ Episode 1/1500, epsilon= 0.9896564593129186, reward = 3.0"
     ]
    }
   ],
   "source": [
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"Experiments/Atari_experiments/\")\n",
    "\n",
    "episode_average_reward = []\n",
    "reward_sum =0 \n",
    "count = 0 \n",
    "total_t = 0\n",
    "reward_summary = deep_q_learning(env,\n",
    "                    total_t,\n",
    "                    experiment_dir=experiment_dir,\n",
    "                    num_episodes=1500,\n",
    "                    replay_memory_size=100000,\n",
    "                    replay_memory_init_size=80000,\n",
    "                    update_target_estimator_every=1000,\n",
    "                    epsilon_start=0.99,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    discount_factor=0.9,\n",
    "                    batch_size_per_device=512,\n",
    "                    save_weights_every = 1000,\n",
    "                    number_of_epochs = 4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kfn015lEeLdS"
   },
   "outputs": [],
   "source": [
    "plt.plot(reward_summary[\"episode_rewards\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7DwRip_sXqZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Atari_Breakout.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
