{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "Ydi0qFEfMA2l",
    "outputId": "d7d7d1c6-a5ed-431a-b5ac-cb65453ca48c",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import gym\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import sys\n",
    "import datetime\n",
    "from PIL import Image  \n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "tf.autograph.set_verbosity(2)\n",
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-QrX45aMhXe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")\n",
    "print(env.unwrapped.get_action_meanings())\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m9ajWOrjMkxP"
   },
   "outputs": [],
   "source": [
    "def state_processer(input_):\n",
    "    input_ = tf.image.rgb_to_grayscale(input_, name=None)\n",
    "    input_ = tf.image.crop_to_bounding_box(input_,34, 0, 160, 160)\n",
    "    input_ = tf.image.resize(input_,(84,84),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return tf.squeeze(input_)\n",
    "  \n",
    "class My_Model(Model):\n",
    "    def __init__(self):\n",
    "        super(My_Model, self).__init__()\n",
    "        self.conv1 = Conv2D(8, 5,strides=(2,2) ,activation='relu')\n",
    "        self.conv2 = Conv2D(16, 3,strides=(1,1) ,activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(1024, activation='relu')\n",
    "        self.d2 = Dense(256,activation='relu')\n",
    "        self.d3 = Dense(len(VALID_ACTIONS))\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        return self.d3(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JeFSMlOwhbTy"
   },
   "outputs": [],
   "source": [
    "#defining loss and optimizers\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "optimizer = tf.keras.optimizers.RMSprop(0.00025)\n",
    "\n",
    "\n",
    "def loss_function(target_y, predicted_y):\n",
    "    return tf.keras.losses.MSE(target_y, predicted_y)\n",
    "  \n",
    "# @tf.function\n",
    "def train_step(model,images, labels, actions):\n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values = model(images, training=True)\n",
    "        predictions = []\n",
    "        for i in range(images.shape[0]):\n",
    "            predictions.append(q_values[i,actions[i]])\n",
    "        loss = loss_function(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss,gradients\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LApMIkC3t_-2"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(estimator, state, epsilon,env):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        state: processed state \n",
    "    Returns:\n",
    "        Action based on the epsilon greedy policy\n",
    "    \"\"\"\n",
    "    x = random.uniform(0, 1)\n",
    "    if x < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        state = tf.cast(np.reshape(state, (1,84,84,4)), tf.float32)\n",
    "        q_values = np.array(estimator(state))\n",
    "        # print(\"Everything's working\")\n",
    "    return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pnSvaajshiFR"
   },
   "outputs": [],
   "source": [
    "\n",
    "def deep_q_learning(env,\n",
    "                    total_t,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=100000,\n",
    "                    replay_memory_init_size=80000,\n",
    "                    update_target_estimator_every=1500,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=300,\n",
    "                    number_of_epochs = 16,\n",
    "                    load_weights_from_checkpoint = 0\n",
    "                    ):\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "    # Keeps track of useful statistics\n",
    "    # like episode length, episode reward\n",
    "    stats = {\"episode_lengths\":np.zeros(num_episodes), \"episode_rewards\":np.zeros(num_episodes)}\n",
    "    # average episode reward for past n episodes\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_path =os.path.abspath(os.path.join(experiment_dir, \"checkpoint/cp.ckpt\"))\n",
    "    summary_path   = os.path.join(experiment_dir, \"summary\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    reward_file_path = os.path.abspath(os.path.join(experiment_dir,\"Rewards.txt\"))\n",
    "    \n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "    if not os.path.exists(summary_path):\n",
    "        os.makedirs(summary_path)\n",
    "        \n",
    "    target_estimator = My_Model()\n",
    "    q_estimator = My_Model()\n",
    "\n",
    "\n",
    "    # Save the weights using the `checkpoint_path` format\n",
    "#     q_estimator.save_weights(checkpoint_path) \n",
    "    if load_weights_from_checkpoint:\n",
    "        print(\"Loading Weights from last checkpoint\")\n",
    "        q_estimator.load_weights(checkpoint_path)\n",
    "            \n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "    \n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = tf.image.convert_image_dtype(state, 'float32', saturate=False, name=None)\n",
    "    state = state_processer(state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action = epsilon_greedy_policy(q_estimator, state, epsilons[min(total_t,epsilon_decay_steps-1)],env)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = tf.image.convert_image_dtype(next_state, 'float32', saturate=False, name=None)\n",
    "        next_state = state_processer(next_state)\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        print(\"\\rprogress = {} %\".format((i/replay_memory_init_size)*100), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processer(state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "    print(\"Starting the Q Learning Algorithm....\")\n",
    "\n",
    "    # Record videos\n",
    "    env= gym.wrappers.Monitor(env,\n",
    "                 directory=monitor_path,\n",
    "                 resume=True,\n",
    "                 video_callable=lambda count: total_t % record_video_every == 0)\n",
    "    \n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        av_loss = 0 \n",
    "      # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = tf.image.convert_image_dtype(state, 'float32', saturate=False, name=None)\n",
    "        state = state_processer(state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        # loss = None\n",
    "         # One step in the environment\n",
    "        for t in itertools.count():\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, epsilon= {}, reward = {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes,epsilon,stats['episode_rewards'][i_episode]), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step in the environment\n",
    "            action = epsilon_greedy_policy(q_estimator, state, epsilon,env)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = tf.image.convert_image_dtype(next_state, 'float32', saturate=False, name=None)\n",
    "            next_state = state_processer(next_state)\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            #Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "            # Update statistics\n",
    "            stats['episode_rewards'][i_episode] += reward\n",
    "            stats['episode_lengths'][i_episode] = t\n",
    "\n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "            \n",
    "            #Calculate q values and targets\n",
    "            q_values_next = target_estimator.predict(next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "            \n",
    "            #training step\n",
    "            for epoch in range(number_of_epochs): \n",
    "                loss, gradients = train_step(q_estimator,states_batch,targets_batch,action_batch)\n",
    "            av_loss += loss.numpy()\n",
    "            total_t += 1\n",
    "    \n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                q_estimator.save_weights(checkpoint_path)\n",
    "                target_estimator.load_weights(checkpoint_path)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state \n",
    "        file_object = open('rewards.txt', 'a')\n",
    "        file_object.write(str(stats['episode_rewards'][i_episode]) + ',' +str(epsilon) + ','+ str(stats['episode_lengths'][i_episode]) + ',' + str(av_loss/stats['episode_lengths'][i_episode]) +',' +  str(total_t)+ '\\n')\n",
    "        file_object.close()\n",
    "    q_estimator.summary()\n",
    "    # Display the model's architecture\n",
    "    return stats\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "Pw-hc16SYmcQ",
    "outputId": "0c52057b-01a5-4f6c-8080-f8335566d8e1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n",
      "progress = 99.99875 %9999999 %%%%Starting the Q Learning Algorithm....\n",
      "Step 82 (80264) @ Episode 313/4000, epsilon= 0.9097028871286089, reward = 0.00"
     ]
    }
   ],
   "source": [
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"Experiments/Atari_experiments/\")\n",
    "\n",
    "episode_average_reward = []\n",
    "reward_sum =0 \n",
    "count = 0 \n",
    "total_t = 0\n",
    "reward_summary = deep_q_learning(env,\n",
    "                    total_t,\n",
    "                    experiment_dir=experiment_dir,\n",
    "                    num_episodes=4000,\n",
    "                    replay_memory_size=100000,\n",
    "                    replay_memory_init_size=80000,\n",
    "                    update_target_estimator_every=4000,\n",
    "                    epsilon_start= 1,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=800000,\n",
    "                    discount_factor=0.99,\n",
    "                    batch_size=32,\n",
    "                    number_of_epochs = 1, \n",
    "                    load_weights_from_checkpoint = 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kfn015lEeLdS"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reward_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-696e0347a822>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_summary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"episode_rewards\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reward_summary' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(reward_summary[\"episode_rewards\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7DwRip_sXqZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Atari_Breakout.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
